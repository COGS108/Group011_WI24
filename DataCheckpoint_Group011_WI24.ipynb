{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you lost points on the last checkpoint you can get them back by responding to TA/IA feedback**  \n",
    "\n",
    "Update/change the relevant sections where you lost those points, make sure you respond on GitHub Issues to your TA/IA to call their attention to the changes you made here.\n",
    "\n",
    "Please update your Timeline... no battle plan survives contact with the enemy, so make sure we understand how your plans have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Sheena Patel\n",
    "- Anna Chen\n",
    "- Shreya Velagala\n",
    "- Catherine Du\n",
    "- Esther Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> How does the relationship between TikTok fashion trends and their popularity as Google search terms inform the future of those trends? </b> <br>\n",
    "\n",
    "In Depth RQ: <br>\n",
    "How can we predict the level of interest in various TikTok fashion trends across New York and California using TikTok trend interest score data from January 1st, 2020 to December 31st, 2023 using a model that inputs the fashion trend name, monthly time data, and region to predict an interest rating between 0 - 100 and an associated label of 'low' (interest score < 25), 'rising' (interest score < 50), 'popular' (interest score < 75), and 'trending (interest score > 75) for the specified input in 2024? <br>\n",
    "\n",
    "This question aims to develop a predictive model that evaluates the popularity of TikTok fashion trends in different regions and times, using a quantifiable interest rating system. The focus on a state-by-state analysis allows for a detailed understanding of regional preferences and trends overtime. <br>\n",
    "\n",
    "We specifically focus on New York and California because they are the top two most populous American states and have the highest TikTok usage in the United States. When gathering data from Google Trends, we had to select states that displayed enough search queries for us to use.<br>\n",
    "\n",
    "Interest Score/Interest Over Time Definition:  <br>\n",
    "The \"interest score\" on Google Trends represents the relative popularity of a search query in a\n",
    "specific region and time frame. It is indexed from 0 to 100, where 100 signifies the peak popularity\n",
    "for the term. This score does not indicate the absolute search volume but rather shows the search term's popularity relative to the highest point on the chart for the given region and time. A higher score means more people are searching for that particular term at that time, while a lower score indicates lesser interest. The data is useful for identifying trends and understanding how interest in certain topics changes over time. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team is curious to understand how current fashion trends can be understood to predict the rating of a piece of clothing. We aim to create a prediction model that can take types of clothing and dates as input to predict the rating of a piece of clothing. We hope this model will have real-world application by possibly allowing companies to predict ratings for different clothing pieces based on GenZ fashion to potentially improve clothing purchase rates. Many datasets exist that contain information about women’s clothing including product descriptions, reviews, and ratings. However, we would like to incorporate TikTok trends into our dataset to use TikTok fashion trends as a data feature when predicting the rating for clothing. \n",
    "\n",
    "Currently, we have a few datasets including Amazon Women's Fashion Dataset<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) and Women's Ecommerce Clothing Reviews<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) that seem relevant to our project goals. Based on our initial research, our ideal dataset would include columns for clothing type, clothing description, clothing review, clothing rating, and a label for each row representing what type of TikTok trend the clothing item falls into based on the clothing description. Additionally, we found other projects that have approached similar problems. One example is this project: Amazon Women's Clothing Review<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1). In the project, the researcher performs EDA by analyzing various trends in the sentiment for Amazon clothing reviews. We plan to incorporate a similar approach to our EDA by visualizing the trends in rating prediction, but analyzing prediction from the perspective of TikTok trends and views for different TikTok trends influencing rating. Another project that had a similar approach is Rating prediction<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3). This project performs rating prediction similar to what we would like to do. My team plans to use this project as an example when creating our prediction model and also use a similar approach for EDA using TF-IDF to assign TikTok trends to different clothing descriptions. \n",
    "\n",
    "The first project<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3) found that the main trigrams from the reviews fall into the positive sentiment categories. For instance, the research found that fit true size, run true size, fit just right, love love love, fit like glove, usual wear size, and every time wear were some of the most prominent trigrams. This research conducts more of a sentiment analysis to show that reviews and ratings are more positive in their dataset. The second project<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) performed EDA and tried to analyze what kinds of age groups make what kinds of reviews and ratings. The research project found that layering, lounge and swim clothing tend to have the best reviews. The research also interestingly found that higher age groups had worse ratings. What we found interesting about this project is that TF-IDF Vectorization was used to convert the clothing descriptions into vectors to be passed as input to the prediction model. I think we are going to have to take a similar approach to our project.\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Jaewook. (2022, October 13). Amazon reviews on Women Dresses. Kaggle. https://www.kaggle.com/code/jaewook704/amazon-reviews-on-women-dresses\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Agarap, Abien Fred, and Nicapotato. (2018, Feburary 3). Women’s e-Commerce Clothing Reviews. Kaggle. https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3) Matteoanzano. (2023, December 5). Customer review analysis with text mining. Kaggle. https://www.kaggle.com/code/matteoanzano111/customer-review-analysis-with-text-mining/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a few possible hypotheses under exploration:\n",
    "- TikTok trends that were more popular towards the end of quarantine (2019/2020), which we have data for, will be less popular in 2024 \n",
    "- TikTok trends that were less popular in the end of quarantine (2019/2020), which we have data for, may be more popular in 2024. \n",
    "- TikTok trends that involve more seasonal clothing like festive clothing for Christmas or summer clothing like dresses and skirts will be popular in their respective seasons in 2024. For instance, skirts which are summer clothing pieces are more likley to be popular in summer 2024, while sweaters which are winter clothing items, are more likely to be popular in winter 2024. \n",
    "    - Trends with bright colors are more likely to have higher interest scores in the summer than in other seasons. \n",
    "    - Trends with knit material will have higher interest scores in winter and cotton material will have higher interest scores in summer.\n",
    "    - Trends with more blouses will have higher interest scores in the summer and sweaters will have higher interest scores in winter. \n",
    "- TikTok trends that are considered to be the most popular of all time will continue to be very popular and have high interest scores throughout 2024. \n",
    "- TikTok trends that have been endorsed by celebrities and thus have had high interest scores in the past will continue to have high interest scores in 2024. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "We compiled a list of the top 30 TikTok trends from 1/1/2020 through 12/31/2023 using ChatGPT and external sources <a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) <a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) and used Google Trends to search up these trend names and understand how the interest score rating changed overtime for them. There, we observed the data for “Interest over time”—which represents search interest (a value of 0-100 to represent peak popularity) relative to the highest point on the chart for the given region (the United States) and time (our aforementioned time frame)---where users searching for this trend name also searched with related queries. We collected 30 datasets by searching up 30 different top TikTok fashion trends on Google Trends to form our overall dataset of ~6500 datapoints. <br>\n",
    "\n",
    "\n",
    "We describe the datasets below. We will be combining all of these datasets using pd.concat and then cleaning all the merged data together. The datasets will be stacked in rows along axis = 1 and then cleaned. The code for cleaning and combining the datasets is below the dataset descriptions. We use the [Pytrends API](https://pypi.org/project/pytrends/) to gather each dataset in a for loop into a separate data structure and then combine all the datasets using a dataframe at the very end to be cleaned. <br>\n",
    "\n",
    "Data Cleaning Procedure: <br>\n",
    "The data that we began with was relatively tidy because we made sure to choose states ('CA' and 'NY) which have higher populations and higher TikTok usage so there were no null values for the interest score for the fashion trends we searched up. We initially also chose Texas as a state in our research, but Texas gave us nearly 3000 null values because the fashion trends were not searched enough within the time frame we chose to actually have interest scores on Google Trends, so we removed Texas from our analysis. We also did some pre-processing steps where for each TikTok trend we found, we searched up the keyword on Google Trends and saw whether the interest score graphs had an updward, downward, or rising and falling trend existed for the interest score on Google Trends. We removed all fashion trends that lacked a relationship between interest score overtime. We also removed all fashion trends that gave an error saying not enough values to be displayed. The main data cleaning we had to do involved merging the different datasets together and cleaning the data to be appropriate to pass into a Machine Learning model for prediction. Here are the various data cleaning steps we took: <br>\n",
    "1. Merge all 30 datasets together \n",
    "2. Use .unique() to make sure all 30 trends were added to the dataframe  \n",
    "3. Check if any null values exist, remove all data for fashion trends with null values \n",
    "4. Make sure we have >100 observations for each fashion trend, if not, expand the timeframe \n",
    "5. Rename the columns to be more clean \n",
    "6. Apply a function to make a column that transforms each interest score observation into a label: 'low', 'rising', 'popular', 'trending' so that these labels can be predicted along with the interest score \n",
    "7. Make sure labels have been assigned correctly \n",
    "8. Create a new column that converts dates to numeric months for the ML model \n",
    "9. Perform one hot encoding for the trends to be used for the ML model \n",
    "10. Remove the 'date' and 'trend' columns because they are no longer needed\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Howell, Carolyn. \"TikTok Fashion Trends 2023: Unveiling the Hottest Styles.\" High Social, 31 Aug. 2023, www.highsocial.com/resources/tiktok-fashion-trends-2023-unveiling-the-hottest-styles/.\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) \"Top TikTok Fashion Trends of 2023 (So Far).\" Sweety High, www.sweetyhigh.com/read/top-tiktok-fashion-trends-2023-040323. Accessed 23 Feb. 2024.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the datasets, we manually checked the distributions generated by google trends for each trend to check if there was enough data and to see if any of them has cyclic patterns. The google docs here contains the distribution of all the trends we ended up using: [Trend Distributions](https://docs.google.com/document/d/13n-aXE2DKkp8noNjx5u7LjmYcoc4Xv3isw8XZL0vnSA/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Dataset #1: Y2K </b><br>\n",
    "Keyword: Y2K -> Keyword for one of the top TikTok Fashion Trends between 2019 and 2023<br>\n",
    "Dataset Name: Y2K <br>\n",
    "Link to the dataset: [Google Trends Keyword Y2K](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=y2k&hl=en)<br>\n",
    "Number of observations: 209<br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))<br>\n",
    "Description: The dataset, gathered using Pytrends API by searching the keyword 'Y2K' on Google Trends, is structured in a dataframe. It includes metrics like interest scores over time, with data types encompassing datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is required for converting datetime to integer month values and creating columns for trends using one-hot encoding. This preprocessing is essential for feeding the trends and months into a Machine Learning prediction model. Additionally, interest scores will be categorized into labels such as 'low', 'rising', 'popular', or 'trending' for model output. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #2: Cottagecore </b>\n",
    "\n",
    "Keyword: Cottagecore -> Keyword for one of the top TikTok Fashion Trends between 2019 and 2023<br>\n",
    "Dataset Name: Cottagecore<br>\n",
    "Link to the dataset: [Google Trends Keyword Cottagecore](https://www.google.com/url?q=https://trends.google.com/trends/explore?date%3D2020-01-01%25202023-12-31%26geo%3DUS%26q%3Dcottagecore%26hl%3Den&sa=D&source=docs&ust=1708745400653624&usg=AOvVaw0LOapYV77MsRKuz7RaC-Y8)<br>\n",
    "Number of observations: 209<br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The dataset, gathered using Pytrends API by searching the keyword 'Cottagecore' on Google Trends, is structured in a dataframe. It includes metrics like interest scores over time, with data types encompassing datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is required for converting datetime to integer month values and creating columns for trends using one-hot encoding. This preprocessing is essential for feeding the trends and months into a Machine Learning prediction model. Additionally, interest scores will be categorized into labels such as 'low', 'rising', 'popular', or 'trending' for model output. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #3: E-Girl </b>\n",
    "\n",
    "Keyword: E-Girl -> Keyword for one of the top TikTok Fashion Trends between 2019 and 2023<br>\n",
    "Dataset Name: E-Girl<br>\n",
    "Link to the dataset: [Google Trends Keyword E-Girl](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=e-girl&hl=en)<br>\n",
    "Number of observations: 209<br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))<br>\n",
    "Description: The dataset, gathered using Pytrends API by searching the keyword 'E-Girl' on Google Trends, is structured in a dataframe. It includes metrics like interest scores over time, with data types encompassing datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is required for converting datetime to integer month values and creating columns for trends using one-hot encoding. This preprocessing is essential for feeding the trends and months into a Machine Learning prediction model. Additionally, interest scores will be categorized into labels such as 'low', 'rising', 'popular', or 'trending' for model output. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #4: Vintage Thrift </b>\n",
    "\n",
    "Keyword: Vintage Thrift  -> Keyword for one of the top TikTok Fashion Trends between 2019 and 2023<br>\n",
    "Dataset Name: Vintage Thrift <br>\n",
    "Link to the dataset: [Google Trends Keyword Vintage Thrift](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=vintage%20thrift&hl=en)<br>\n",
    "Number of observations: 209<br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))<br>\n",
    "Description: The dataset, gathered using Pytrends API by searching the keyword 'Vintage Thrift' on Google Trends, is structured in a dataframe. It includes metrics like interest scores over time, with data types encompassing datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is required for converting datetime to integer month values and creating columns for trends using one-hot encoding. This preprocessing is essential for feeding the trends and months into a Machine Learning prediction model. Additionally, interest scores will be categorized into labels such as 'low', 'rising', 'popular', or 'trending' for model output. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #5: Fairycore  </b>\n",
    "\n",
    "Keyword: Fairycore -> Keyword for one of the top TikTok Fashion Trends between 2019 and 2023<br>\n",
    "Dataset Name: Fairycore<br>\n",
    "Link to the dataset: [Google Trends Keyword Fairycore](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=fairycore&hl=en)<br>\n",
    "Number of observations: 209<br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The dataset, gathered using Pytrends API by searching the keyword 'Fairycore' on Google Trends, is structured in a dataframe. It includes metrics like interest scores over time, with data types encompassing datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is required for converting datetime to integer month values and creating columns for trends using one-hot encoding. This preprocessing is essential for feeding the trends and months into a Machine Learning prediction model. Additionally, interest scores will be categorized into labels such as 'low', 'rising', 'popular', or 'trending' for model output. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #6: Vanilla Girl </b>\n",
    "\n",
    "Keyword: Vanilla Girl -> Keyword for one of the top TikTok Fashion Trends between 2019 and 2023<br>\n",
    "Dataset Name: Vanilla Girl <br> \n",
    "Link to the dataset: [Google Trends Keyword Vanilla Girl](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=vanilla%20girl&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California)) <br>\n",
    "Description: The dataset, gathered using Pytrends API by searching the keyword 'Vanilla Girl' on Google Trends, is structured in a dataframe. It includes metrics like interest scores over time, with data types encompassing datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is required for converting datetime to integer month values and creating columns for trends using one-hot encoding. This preprocessing is essential for feeding the trends and months into a Machine Learning prediction model. Additionally, interest scores will be categorized into labels such as 'low', 'rising', 'popular', or 'trending' for model output. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #7: Clean Girl Aesthetic </b>\n",
    "\n",
    "Keyword: Clean Girl Aesthetic -> Keyword for one of the top TikTok Fashion Trends between 2019 and 2023 <br>\n",
    "Dataset Name: Clean Girl Aesthetic <br>\n",
    "Link to the dataset: [Google Trends Keyword Clean Girl Aesthetic](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=clean%20girl%20aesthetic&hl=en)  <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The dataset, sourced using Pytrends API by searching 'Clean Girl Aesthetic' on Google Trends, is formatted as a dataframe. It features metrics such as interest scores over time, including datetime, integer scores, and regional strings ('CA', 'NY'). It requires data cleaning for transforming datetime into integer month values and for the creation of trend columns using one-hot encoding. This process is vital for integrating the trends and months into a Machine Learning prediction model. Further, the interest scores will be labeled as 'low', 'rising', 'popular', or 'trending' for the model's output. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #8: Blokecore  </b>\n",
    "\n",
    "Keyword: Blokecore -> Keyword for one of the top TikTok Fashion Trends between 2019 and 2023\n",
    "Dataset Name: Blokecore <br>\n",
    "Link to the dataset: [Google Trends Keyword Blokecore](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=clean%20girl%20aesthetic&hl=en)   <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The dataset, compiled using the Pytrends API with 'Blokecore' as the search keyword on Google Trends, is presented in a dataframe format. It includes metrics such as interest scores over time, consisting of datetime, integer scores, and regional strings ('CA', 'NY'). Necessary data cleaning involves converting datetime to integer month values and adding columns for trends via one-hot encoding. This preparation is crucial for incorporating the trends and months into a Machine Learning prediction model. Interest scores will also be classified under labels like 'low', 'rising', 'popular', or 'trending' for the model's output. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #9: Barbie Challenge </b>\n",
    "\n",
    "Keyword: Barbie Challenge -> Top TikTok Fashion Trend between 2019 and 2023 <br>\n",
    "Dataset Name: Barbie Challenge <br>\n",
    "Link to the dataset: [Google Trends Keyword Barbie Challenge](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=barbie%20challenge&hl=en)   <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: This dataset, sourced via Pytrends API with 'Barbie Challenge' from Google Trends, is in a dataframe format. It includes interest scores over time, featuring datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning involves converting datetime to integer month values and creating trend columns using one-hot encoding for integration into a Machine Learning prediction model. Interest scores are labeled as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #10: Shirt Jackets </b>\n",
    "\n",
    "Keyword: Shirt Jackets -> Top TikTok Fashion Trend between 2019 and 2023 <br>\n",
    "Dataset Name: Shirt Jackets <br>\n",
    "Link to the dataset: [Google Trends Keyword Shirt Jackets](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=shirt%20jackets&hl=en)  <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: Gathered using the Pytrends API, the 'Shirt Jackets' dataset from Google Trends is in dataframe form. It contains metrics like interest scores, datetime, integer scores, and regional strings ('CA', 'NY'). The data requires cleaning for datetime conversion into integer months and for trend columns addition via one-hot encoding, essential for Machine Learning prediction model input. Interest scores will be classified as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #11: Balletcore </b>\n",
    "\n",
    "Keyword: Balletcore -> Top TikTok Fashion Trend between 2019 and 2023 <br>\n",
    "Dataset Name: Balletcore <br>\n",
    "Link to the dataset: [Google Trends Keyword Balletcore](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Balletcore&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Balletcore dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #12: Coastal Grandmother </b>\n",
    "\n",
    "Keyword: Coastal Grandmother -> Top TikTok Fashion Trend between 2019 and 2023 <br>\n",
    "Dataset Name: Coastal Grandmother <br>\n",
    "Link to the dataset: [Google Trends Keyword Coastal Grandmother](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Coastal%20Grandmother&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Coastal Grandmother dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #13: Gingham </b>\n",
    "\n",
    "Keyword: Gingham -> Top TikTok Fashion Trend between 2019 and 2023 <br>\n",
    "Dataset Name: Gingham <br>\n",
    "Link to the dataset: [Google Trends Keyword Gingham](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Gingham&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Gingham dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #14: Maxi Skirts </b>\n",
    "\n",
    "Keyword: Maxi Skirts -> Top TikTok Fashion Trend between 2019 and 2023 <br>\n",
    "Dataset Name: Maxi Skirts <br>\n",
    "Link to the dataset: [Google Trends Keyword Maxi Skirts](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Maxi%20Skirts&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Maxi Skirts dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #15: Corset </b>\n",
    "\n",
    "Keyword: Corset  <br>\n",
    "Dataset Name: Corset <br>\n",
    "Link to the dataset: [Google Trends Keyword Corset](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Corset&hl=en)  <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Corset dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #16: Leg Warmers </b>\n",
    "\n",
    "Keyword: Leg Warmers <br>\n",
    "Dataset Name: Leg Warmers <br>\n",
    "Link to the dataset: [Google Trends Keyword Leg Warmers](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Leg%20Warmers&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Leg Warmers dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #17: Birkenstocks </b>\n",
    "\n",
    "Keyword: Birkenstocks  <br>\n",
    "Dataset Name: Birkenstocks <br>\n",
    "Link to the dataset: [Google Trends Keyword Birkenstocks](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Birkenstocks&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Birkenstocks dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #18: Cloud Slides </b> <br>\n",
    "\n",
    "Keyword: Cloud Slides <br>\n",
    "Dataset Name: Cloud Slides <br>\n",
    "Link to the dataset: [Google Trends Keyword Cloud Slides](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Cloud%20Slides&hl=en) <br>\n",
    "Number of observations: 209  <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Cloud Slides dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #19: Leather </b>\n",
    "\n",
    "Keyword: Leather <br>\n",
    "Dataset Name: Leather  <br>\n",
    "Link to the dataset: [Google Trends Keyword Leather](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Leather&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Leather dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #20: Funky Pants </b>\n",
    "\n",
    "Keyword: Funky Pants <br>\n",
    "Dataset Name: Funky Pants <br>\n",
    "Link to the dataset: [Google Trends Keyword Funky Pants](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Funky%20Pants&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Funky Pants dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #21: Sweater Vests </b>\n",
    "\n",
    "Keyword: Sweater Vests <br>\n",
    "Dataset Name: Sweater Vests <br>\n",
    "Link to the dataset: [Google Trends Keyword Sweater Vests]() <br>\n",
    "Number of observations: 209  <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Sweater%20Vests&hl=enYork, California))\n",
    "\n",
    "Description: The Sweater Vests dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #22: Linen Pants </b>\n",
    "\n",
    "Keyword: Linen Pants <br>\n",
    "Dataset Name: Linen Pants <br>\n",
    "Link to the dataset: [Google Trends Keyword Linen Pants](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Linen%20Pants&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Linen Pants dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #23: Tube Tops </b>\n",
    "\n",
    "Keyword: Tube Tops <br>\n",
    "Dataset Name: Tube Tops <br>\n",
    "Link to the dataset: [Google Trends Keyword Tube Tops](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Tube%20Tops&hl=en)  <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Tube Tops dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #24: Baggy pants </b>\n",
    "\n",
    "Keyword: Baggy pants <br>\n",
    "Dataset Name: Baggy pants  <br>\n",
    "Link to the dataset: [Google Trends Keyword Baggy pants](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Baggy%20pants&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Baggy pants dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #25: Low-rise </b>\n",
    "\n",
    "Keyword: Low-rise <br>\n",
    "Dataset Name: Low-rise <br> \n",
    "Link to the dataset: [Google Trends Keyword Low-rise](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Low-rise&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Low-rise dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #26: Crochet  </b>\n",
    "\n",
    "Keyword: Crochet <br>\n",
    "Dataset Name: Crochet <br>\n",
    "Link to the dataset: [Google Trends Keyword Crochet](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Crochet&hl=en) <br>\n",
    "Number of observations: 209  <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Crochet dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #27: Platform Sandals </b>\n",
    "\n",
    "Keyword: Platform Sandals <br>\n",
    "Dataset Name: Platform Sandals <br>\n",
    "Link to the dataset: [Google Trends Keyword Platform Sandals](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Platform%20Sandals&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Platform Sandals dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #28: Tomato Girl </b>\n",
    "\n",
    "Keyword: Tomato Girl <br>\n",
    "Dataset Name: Tomato Girl <br>\n",
    "Link to the dataset: [Google Trends Keyword Tomato Girl](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Tomato%20Girl&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Tomato Girl dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #29: Soft Girl Aesthetic </b>\n",
    "\n",
    "Keyword: Soft Girl Aesthetic <br>\n",
    "Dataset Name: Soft Girl Aesthetic <br>\n",
    "Link to the dataset: [Google Trends Keyword Soft Girl Aesthetic](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Soft%20Girl&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Soft Girl Aesthetic dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>\n",
    "\n",
    "\n",
    "<b> Dataset #30: Mermaid Core </b>\n",
    "\n",
    "Keyword: Mermaid Core <br>\n",
    "Dataset Name: Mermaid Core <br>\n",
    "Link to the dataset: [Google Trends Keyword Mermaid Core](https://trends.google.com/trends/explore?date=2020-01-01%202023-12-31&geo=US&q=Mermaid%20Core&hl=en) <br>\n",
    "Number of observations: 209 <br>\n",
    "Number of variables: 2 (Interest Score, Trend, 2 regions (New York, California))\n",
    "<br>\n",
    "Description: The Mermaid Core dataset, obtained through Pytrends API from Google Trends, is structured in a dataframe. It tracks interest scores over time and includes datetime, integer scores, and regional strings ('CA', 'NY'). Data cleaning is necessary to transform datetime into integer months and to create trend-specific columns through one-hot encoding. This process aids in preparing the data for a Machine Learning prediction model. Interest scores are to be categorized as 'low', 'rising', 'popular', or 'trending'. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytrends in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (4.9.2)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from pytrends) (2.31.0)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from pytrends) (2.2.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from pytrends) (5.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from pandas>=0.25->pytrends) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from pandas>=0.25->pytrends) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from requests>=2.0->pytrends) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from requests>=2.0->pytrends) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from requests>=2.0->pytrends) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from requests>=2.0->pytrends) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\windy\\documents\\github\\joeclone\\group011_wi24\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION\n",
    "%pip install pytrends\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant Imports\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Top 30 TikTok trends that we will iterate through\n",
    "TikTokTrends = [\"Y2K\", \"Cottagecore\", \"E-Girl\", \"Vintage Thrift\", \"Fairycore\", \"Vanilla Girl\",\n",
    "                \"Clean Girl Aesthetic\", \"Blokecore\", \"Barbie Challenge\", \"Shirt Jackets\", \"Balletcore\",\n",
    "               \"Coastal Grandmother\", \"Gingham\", \"Maxi Skirts\", \"Corset\", \"Leg Warmers\", \"Birkenstocks\", \"Cloud Slides\",\n",
    "               \"Leather\", \"Funky Pants\", \"Sweater Vests\", \"Linen Pants\", \"Tube Tops\", \"Baggy pants\", \"Low-rise\", \"Crochet\",\n",
    "               \"Platform Sandals\", \"Tomato Girl\", \"Soft Girl Aesthetic\", \"Mermaid Core\"]\n",
    "# Multiple dataframes where one dataframe per trend will be in this list\n",
    "df_per_trend = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y2K']\n",
      "['Cottagecore']\n",
      "['E-Girl']\n",
      "['Vintage Thrift']\n",
      "['Fairycore']\n",
      "['Vanilla Girl']\n",
      "['Clean Girl Aesthetic']\n",
      "['Blokecore']\n",
      "['Barbie Challenge']\n",
      "['Shirt Jackets']\n",
      "['Balletcore']\n",
      "['Coastal Grandmother']\n",
      "['Gingham']\n",
      "['Maxi Skirts']\n",
      "['Corset']\n",
      "['Leg Warmers']\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='trends.google.com', port=443): Read timed out. (read timeout=2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connection.py:653\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 653\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connection.py:806\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    804\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 806\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1075\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: _ssl.c:985: The handshake operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\util\\util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:469\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:370\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='trends.google.com', port=443): Read timed out. (read timeout=2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m geo \u001b[38;5;129;01min\u001b[39;00m geo_locations:\n\u001b[0;32m     22\u001b[0m     pytrends\u001b[38;5;241m.\u001b[39mbuild_payload(kw_list, cat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, timeframe\u001b[38;5;241m=\u001b[39mtimeframe, geo\u001b[38;5;241m=\u001b[39mgeo, gprop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpytrends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterest_over_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     25\u001b[0m         trends_data[geo] \u001b[38;5;241m=\u001b[39m data[trend]\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\pytrends\\request.py:232\u001b[0m, in \u001b[0;36mTrendReq.interest_over_time\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m over_time_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# convert to string as requests will mangle\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtz\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtz\n\u001b[0;32m    229\u001b[0m }\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# make the request and parse the returned json\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m req_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTEREST_OVER_TIME_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrim_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mover_time_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(req_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimelineData\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (df\u001b[38;5;241m.\u001b[39mempty):\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\pytrends\\request.py:140\u001b[0m, in \u001b[0;36mTrendReq._get_data\u001b[1;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     response \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mpost(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    137\u001b[0m                       cookies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    138\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_args)  \u001b[38;5;66;03m# DO NOT USE retries or backoff_factor here\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequests_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# DO NOT USE retries or backoff_factor here\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# check if the response contains json and throw an exception otherwise\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Google mostly sends 'application/json' in the Content-Type header,\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# but occasionally it sends 'application/javascript\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# and sometimes even 'text/javascript\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \\\n\u001b[0;32m    147\u001b[0m         response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/javascript\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# some responses start with garbage characters, like \")]}',\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# these have to be cleaned before being passed to the json parser\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\windy\\Documents\\GitHub\\JoeClone\\Group011_WI24\\.venv\\Lib\\site-packages\\requests\\adapters.py:532\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='trends.google.com', port=443): Read timed out. (read timeout=2)"
     ]
    }
   ],
   "source": [
    "# For loop that iterates through all top 27 TikTok Trends from 2020 to 2023 and populates a\n",
    "# list (df_per_trend) with interest scores for each trend.\n",
    "for trend in TikTokTrends:\n",
    "    # google trends df\n",
    "    # Initialize pytrends\n",
    "    pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "    # Define the keyword and timeframe\n",
    "    kw_list = []\n",
    "    kw_list.append(trend)\n",
    "    print(kw_list)\n",
    "    timeframe = '2020-01-01 2023-12-31'\n",
    "\n",
    "    # Define geographic locations\n",
    "    geo_locations = ['US-CA', 'US-NY']  # California, New York, DEFAULT (ALL of US when state not specified)\n",
    "\n",
    "    # Dictionary to hold data\n",
    "    trends_data = {}\n",
    "\n",
    "    # Fetching the data for each location\n",
    "    for geo in geo_locations:\n",
    "        pytrends.build_payload(kw_list, cat=0, timeframe=timeframe, geo=geo, gprop='')\n",
    "        data = pytrends.interest_over_time()\n",
    "        if not data.empty:\n",
    "            trends_data[geo] = data[trend]\n",
    "\n",
    "    # Combine data from different regions into one DataFrame\n",
    "    df_curr_trend = pd.concat(trends_data, axis=1)\n",
    "\n",
    "    df_curr_trend = df_curr_trend.reset_index()\n",
    "    df_curr_trend[\"trend\"] = trend\n",
    "    df_per_trend.append(df_curr_trend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging dataframes for all trends together\n",
    "df_all_trends = pd.concat(df_per_trend, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "na = df_all_trends['US-CA'].isna().any()\n",
    "# False\n",
    "na = df_all_trends['US-NY'].isna().any()\n",
    "# False\n",
    "print(na)\n",
    "print(df_all_trends['US-CA'].dtypes)\n",
    "# int64\n",
    "print(df_all_trends['US-NY'].dtypes)\n",
    "# int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>US-CA</th>\n",
       "      <th>US-NY</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  US-CA  US-NY trend\n",
       "0 2020-01-05     13     17   Y2K\n",
       "1 2020-01-12     15     12   Y2K\n",
       "2 2020-01-19     11     15   Y2K\n",
       "3 2020-01-26      9     17   Y2K\n",
       "4 2020-02-02      8     20   Y2K"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_trends.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nY2K                     209\\nCorset                  209\\nCrochet                 209\\nLow-rise                209\\nBaggy pants             209\\nTube Tops               209\\nLinen Pants             209\\nSweater Vests           209\\nFunky Pants             209\\nLeather                 209\\nCloud Slides            209\\nBirkenstocks            209\\nLeg Warmers             209\\nMaxi Skirts             209\\nCottagecore             209\\nGingham                 209\\nCoastal Grandmother     209\\nBalletcore              209\\nShirt Jackets           209\\nBarbie Challenge        209\\nBlokecore               209\\nClean Girl Aesthetic    209\\nVanilla Girl            209\\nFairycore               209\\nVintage Thrift          209\\nE-Girl                  209\\nPlatform Sandals        209\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all_trends)\n",
    "'''\n",
    "5643\n",
    "'''\n",
    "df_all_trends[\"trend\"].value_counts()\n",
    "'''\n",
    "Y2K                     209\n",
    "Corset                  209\n",
    "Crochet                 209\n",
    "Low-rise                209\n",
    "Baggy pants             209\n",
    "Tube Tops               209\n",
    "Linen Pants             209\n",
    "Sweater Vests           209\n",
    "Funky Pants             209\n",
    "Leather                 209\n",
    "Cloud Slides            209\n",
    "Birkenstocks            209\n",
    "Leg Warmers             209\n",
    "Maxi Skirts             209\n",
    "Cottagecore             209\n",
    "Gingham                 209\n",
    "Coastal Grandmother     209\n",
    "Balletcore              209\n",
    "Shirt Jackets           209\n",
    "Barbie Challenge        209\n",
    "Blokecore               209\n",
    "Clean Girl Aesthetic    209\n",
    "Vanilla Girl            209\n",
    "Fairycore               209\n",
    "Vintage Thrift          209\n",
    "E-Girl                  209\n",
    "Platform Sandals        209\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>US-CA_Interest_Score</th>\n",
       "      <th>US-NY_Interest_Score</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>Y2K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  US-CA_Interest_Score  US-NY_Interest_Score trend\n",
       "0 2020-01-05                    13                    17   Y2K\n",
       "1 2020-01-12                    15                    12   Y2K\n",
       "2 2020-01-19                    11                    15   Y2K\n",
       "3 2020-01-26                     9                    17   Y2K\n",
       "4 2020-02-02                     8                    20   Y2K"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming Columns\n",
    "df_all_trends.rename(columns={'US-CA': 'US-CA_Interest_Score', 'US-NY': 'US-NY_Interest_Score'}, inplace=True)\n",
    "df_all_trends.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning to assign labels 'low', 'rising', 'popular', and 'trending' to each datapoint.\n",
    "# This will be the output ofr the prediction\n",
    "def bin_interest_score(num):\n",
    "    if num < 25:\n",
    "        return 'low'\n",
    "    elif num < 50:\n",
    "        return 'rising'\n",
    "    elif num < 75:\n",
    "        return 'popular'\n",
    "    else:\n",
    "        return 'trending'\n",
    "\n",
    "df_all_trends['US-CA_Interest_Label'] = df_all_trends['US-CA_Interest_Score'].apply(bin_interest_score)\n",
    "df_all_trends['US-NY_Interest_Label'] = df_all_trends['US-NY_Interest_Score'].apply(bin_interest_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>US-CA_Interest_Score</th>\n",
       "      <th>US-NY_Interest_Score</th>\n",
       "      <th>trend</th>\n",
       "      <th>US-CA_Interest_Label</th>\n",
       "      <th>US-NY_Interest_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>2023-07-16</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>Low-rise</td>\n",
       "      <td>rising</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>2023-07-23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low-rise</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>2023-07-30</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>Low-rise</td>\n",
       "      <td>rising</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5203</th>\n",
       "      <td>2023-08-06</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>Low-rise</td>\n",
       "      <td>trending</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>2023-08-13</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>Low-rise</td>\n",
       "      <td>rising</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>2023-02-05</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>Platform Sandals</td>\n",
       "      <td>rising</td>\n",
       "      <td>rising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5596</th>\n",
       "      <td>2023-02-12</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>Platform Sandals</td>\n",
       "      <td>rising</td>\n",
       "      <td>rising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5597</th>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>Platform Sandals</td>\n",
       "      <td>rising</td>\n",
       "      <td>rising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5598</th>\n",
       "      <td>2023-02-26</td>\n",
       "      <td>35</td>\n",
       "      <td>55</td>\n",
       "      <td>Platform Sandals</td>\n",
       "      <td>rising</td>\n",
       "      <td>popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5599</th>\n",
       "      <td>2023-03-05</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "      <td>Platform Sandals</td>\n",
       "      <td>rising</td>\n",
       "      <td>rising</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  US-CA_Interest_Score  US-NY_Interest_Score             trend  \\\n",
       "5200 2023-07-16                    32                    13          Low-rise   \n",
       "5201 2023-07-23                     0                     0          Low-rise   \n",
       "5202 2023-07-30                    33                    15          Low-rise   \n",
       "5203 2023-08-06                    80                     0          Low-rise   \n",
       "5204 2023-08-13                    26                    13          Low-rise   \n",
       "...         ...                   ...                   ...               ...   \n",
       "5595 2023-02-05                    29                    32  Platform Sandals   \n",
       "5596 2023-02-12                    42                    28  Platform Sandals   \n",
       "5597 2023-02-19                    45                    37  Platform Sandals   \n",
       "5598 2023-02-26                    35                    55  Platform Sandals   \n",
       "5599 2023-03-05                    45                    42  Platform Sandals   \n",
       "\n",
       "     US-CA_Interest_Label US-NY_Interest_Label  \n",
       "5200               rising                  low  \n",
       "5201                  low                  low  \n",
       "5202               rising                  low  \n",
       "5203             trending                  low  \n",
       "5204               rising                  low  \n",
       "...                   ...                  ...  \n",
       "5595               rising               rising  \n",
       "5596               rising               rising  \n",
       "5597               rising               rising  \n",
       "5598               rising              popular  \n",
       "5599               rising               rising  \n",
       "\n",
       "[400 rows x 6 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_trends.iloc[5200:5600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>US-CA_Interest_Score</th>\n",
       "      <th>US-NY_Interest_Score</th>\n",
       "      <th>trend</th>\n",
       "      <th>US-CA_Interest_Label</th>\n",
       "      <th>US-NY_Interest_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  month  US-CA_Interest_Score  US-NY_Interest_Score trend  \\\n",
       "0 2020-01-05      1                    13                    17   Y2K   \n",
       "1 2020-01-12      1                    15                    12   Y2K   \n",
       "2 2020-01-19      1                    11                    15   Y2K   \n",
       "3 2020-01-26      1                     9                    17   Y2K   \n",
       "4 2020-02-02      2                     8                    20   Y2K   \n",
       "\n",
       "  US-CA_Interest_Label US-NY_Interest_Label  \n",
       "0                  low                  low  \n",
       "1                  low                  low  \n",
       "2                  low                  low  \n",
       "3                  low                  low  \n",
       "4                  low                  low  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the month and create a new column to be a column of numeric months\n",
    "df_all_trends['date'] = pd.to_datetime(df_all_trends['date'])\n",
    "df_all_trends.insert(1, 'month', df_all_trends['date'].dt.month)\n",
    "df_all_trends.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding for Trends\n",
    "TikTokTrends = [\"Y2K\", \"Cottagecore\", \"E-Girl\", \"Vintage Thrift\", \"Fairycore\", \"Vanilla Girl\",\n",
    "                \"Clean Girl Aesthetic\", \"Blokecore\", \"Barbie Challenge\", \"Shirt Jackets\", \"Balletcore\",\n",
    "               \"Coastal Grandmother\", \"Gingham\", \"Maxi Skirts\", \"Corset\", \"Leg Warmers\", \"Birkenstocks\", \"Cloud Slides\",\n",
    "               \"Leather\", \"Funky Pants\", \"Sweater Vests\", \"Linen Pants\", \"Tube Tops\", \"Baggy pants\", \"Low-rise\", \"Crochet\",\n",
    "               \"Platform Sandals\", \"Tomato Girl\", \"Soft Girl Aesthetic\", \"Mermaid Core\"]\n",
    "\n",
    "for trend in TikTokTrends:\n",
    "    df_all_trends[trend] = [1 if value == trend else 0 for value in df_all_trends['trend']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>US-CA_Interest_Score</th>\n",
       "      <th>US-NY_Interest_Score</th>\n",
       "      <th>trend</th>\n",
       "      <th>US-CA_Interest_Label</th>\n",
       "      <th>US-NY_Interest_Label</th>\n",
       "      <th>Y2K</th>\n",
       "      <th>Cottagecore</th>\n",
       "      <th>E-Girl</th>\n",
       "      <th>...</th>\n",
       "      <th>Sweater Vests</th>\n",
       "      <th>Linen Pants</th>\n",
       "      <th>Tube Tops</th>\n",
       "      <th>Baggy pants</th>\n",
       "      <th>Low-rise</th>\n",
       "      <th>Crochet</th>\n",
       "      <th>Platform Sandals</th>\n",
       "      <th>Tomato Girl</th>\n",
       "      <th>Soft Girl Aesthetic</th>\n",
       "      <th>Mermaid Core</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  month  US-CA_Interest_Score  US-NY_Interest_Score trend  \\\n",
       "0 2020-01-05      1                    13                    17   Y2K   \n",
       "1 2020-01-12      1                    15                    12   Y2K   \n",
       "2 2020-01-19      1                    11                    15   Y2K   \n",
       "3 2020-01-26      1                     9                    17   Y2K   \n",
       "4 2020-02-02      2                     8                    20   Y2K   \n",
       "\n",
       "  US-CA_Interest_Label US-NY_Interest_Label  Y2K  Cottagecore  E-Girl  ...  \\\n",
       "0                  low                  low    1            0       0  ...   \n",
       "1                  low                  low    1            0       0  ...   \n",
       "2                  low                  low    1            0       0  ...   \n",
       "3                  low                  low    1            0       0  ...   \n",
       "4                  low                  low    1            0       0  ...   \n",
       "\n",
       "   Sweater Vests  Linen Pants  Tube Tops  Baggy pants  Low-rise  Crochet  \\\n",
       "0              0            0          0            0         0        0   \n",
       "1              0            0          0            0         0        0   \n",
       "2              0            0          0            0         0        0   \n",
       "3              0            0          0            0         0        0   \n",
       "4              0            0          0            0         0        0   \n",
       "\n",
       "   Platform Sandals  Tomato Girl  Soft Girl Aesthetic  Mermaid Core  \n",
       "0                 0            0                    0             0  \n",
       "1                 0            0                    0             0  \n",
       "2                 0            0                    0             0  \n",
       "3                 0            0                    0             0  \n",
       "4                 0            0                    0             0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_trends.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'month', 'US-CA_Interest_Score', 'US-NY_Interest_Score',\n",
       "       'trend', 'US-CA_Interest_Label', 'US-NY_Interest_Label', 'Y2K',\n",
       "       'Cottagecore', 'E-Girl', 'Vintage Thrift', 'Fairycore', 'Vanilla Girl',\n",
       "       'Clean Girl Aesthetic', 'Blokecore', 'Barbie Challenge',\n",
       "       'Shirt Jackets', 'Balletcore', 'Coastal Grandmother', 'Gingham',\n",
       "       'Maxi Skirts', 'Corset', 'Leg Warmers', 'Birkenstocks', 'Cloud Slides',\n",
       "       'Leather', 'Funky Pants', 'Sweater Vests', 'Linen Pants', 'Tube Tops',\n",
       "       'Baggy pants', 'Low-rise', 'Crochet', 'Platform Sandals', 'Tomato Girl',\n",
       "       'Soft Girl Aesthetic', 'Mermaid Core'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_trends.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop date and trend columns\n",
    "df_all_trends = df_all_trends.drop(['date', 'trend'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['month', 'US-CA_Interest_Score', 'US-NY_Interest_Score',\n",
       "       'US-CA_Interest_Label', 'US-NY_Interest_Label', 'Y2K', 'Cottagecore',\n",
       "       'E-Girl', 'Vintage Thrift', 'Fairycore', 'Vanilla Girl',\n",
       "       'Clean Girl Aesthetic', 'Blokecore', 'Barbie Challenge',\n",
       "       'Shirt Jackets', 'Balletcore', 'Coastal Grandmother', 'Gingham',\n",
       "       'Maxi Skirts', 'Corset', 'Leg Warmers', 'Birkenstocks', 'Cloud Slides',\n",
       "       'Leather', 'Funky Pants', 'Sweater Vests', 'Linen Pants', 'Tube Tops',\n",
       "       'Baggy pants', 'Low-rise', 'Crochet', 'Platform Sandals', 'Tomato Girl',\n",
       "       'Soft Girl Aesthetic', 'Mermaid Core'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_trends.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all_trends' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_all_trends\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_all_trends' is not defined"
     ]
    }
   ],
   "source": [
    "df_all_trends.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any biases/privacy/terms of use issues with the data you propsed?\n",
    "- Our data from google trends doesn't contain privacy concerns for the people they collected the data from, as it is a publicly avaliable resource that does not release any info on users. However there are biases in regard to the time of the data we gathered. Though TikTok was released in 2016, the data we are utilizing is only from 2020, which would show results that are highly catered towards recent effects of TikTok, rather than its overall lifespan.\n",
    "\n",
    "Are there potential biases in your dataset(s), in terms of who it composes, and how it was collected, that may be problematic in terms of it allowing for equitable analysis? (For example, does your data exclude particular populations, or is it likely to reflect particular human biases in a way that could be a problem?) How will you set out to detect these specific biases before, during, and after/when communicating your analysis?\n",
    "-  The 2 locations we are looking into are CA and NY, which both have large populations and have high Tiktok usage. It's important to recognize that our dataset represents users who actively engage with TikTok and have consistent internet access, so it does not account for places where trends have little effect. While acknowledging the influence of additional variables such as zoning and access to clothing stores on sales, we hope that our data could provide valuable suggestions for companies aiming to align their clothing offerings with popular social media trends.\n",
    "\n",
    "How will you handle issues you identified?\n",
    "- In order to handle issues related to biases and privacy in our datasets, we will emphasize that the trends we seek to predict and their impacts apply primarily to the states where this data is collected. Additionally, it may be hard to address issues of racial representation in the dataset, or biases towards specific bodies in relation to clothes trends. Although these issues may arise, our best course of action is to research and incorporate ethically sourced data. Our focus on choosing quality datasets from the start will ensure our analysis can be ethically predictive of the population.\n",
    "- For issues with locations, we want to make sure to prefaced out results with the locations we used in order to areas that are less affected by TikTok trends do not rely strongly on our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regular meetings/checkups every Tuesday at 7pm\n",
    "* Use discord and messages as main form of communication\n",
    "* If issues with communication arises, attempt to solve them as a team\n",
    "  * If a team member is non-responsive after that, contact the professor\n",
    "* Divide up responsibilities so that everyone can contribute\n",
    "* Be open when you are struggling to complete a task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/31 | 11 AM  | N/A  | Previous Project Review | \n",
    "| 2/5  | 10 PM  | Previous Project Review | Potential Project Proposal topics | \n",
    "| 2/8  | 7  PM  | Google doc of potential topics and their relevant datasets | Choosing Project Proposal topic. Assigning roles to complete project proposals by deadline |\n",
    "| 2/16 | 2  PM  | Project Proposal | How to approach web scraping and obtaining the datasets that we need. (tiktok api) Discuss assigning roles for the implementation of the data analysis |\n",
    "| 2/21 | 5  PM  | Look into PyTrends API and google trend results from different Tiktok Trends. | Share information and create a sample dataset for 1 trend as the base for the rest of the trends. Assign each team member a role to complete Project Checkpoint #1. |\n",
    "| 2/24 | 3  PM  | Prepared clean data<br> Project Checkpoint #1: <ul><li>revise research question/report</li><li>specify the datasets we will use and why</li><li>explain our data cleaning process</li></ul> | Checkup on progress, make sure everyone is confident in their tasks |\n",
    "| 3/5  | 7  PM  | Each team member's tasks  | Checkup on progress, make sure everyone is confident in their tasks |\n",
    "| 3/12 | 7  PM  | The data prediction model is nearing conclusion, time to analyze findings | Outlining the conclusion of the project. Distributing tasks for the final project deliverable with relevant findings |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
